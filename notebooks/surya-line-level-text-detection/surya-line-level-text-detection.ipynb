{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kAHiJNWe9qA"
   },
   "source": [
    "# Line-level text detection with Surya\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/surya-line-level-text-detection/surya-line-level-text-detection.ipynb)\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>Important note:</b> This notebook requires python >= 3.9. Please make sure that your environment fulfill to this requirement  before running it </div>\n",
    "\n",
    "In this tutorial we will perform line-level text detection using [Surya](https://github.com/VikParuchuri/surya) toolkit and OpenVINO.\n",
    "\n",
    "![line-level text detection](https://github.com/VikParuchuri/surya/blob/master/static/images/excerpt.png?raw=true)\n",
    "\n",
    "[**image source*](https://github.com/VikParuchuri/surya)\n",
    "\n",
    "\n",
    "Model used for line-level text detection based on [Segformer](https://arxiv.org/pdf/2105.15203.pdf). It has the following features:\n",
    "* It is specialized for document OCR. It will likely not work on photos or other images.\n",
    "* It is for printed text, not handwriting.\n",
    "* The model has trained itself to ignore advertisements.\n",
    "* Languages with very different character sets may not work well.\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Fetch test image](#Fetch-test-image)\n",
    "- [Run PyTorch inference](#Run-PyTorch-inference)\n",
    "- [Convert model to OpenVINO Intermediate Representation (IR) format](#Convert-model-to-OpenVINO-Intermediate-Representation-(IR)-format)\n",
    "- [Run OpenVINO model](#Run-OpenVINO-model)\n",
    "- [Apply post-training quantization using NNCF](#Apply-post-training-quantization-using-NNCF)\n",
    "    - [Prepare dataset](#Prepare-dataset)\n",
    "    - [Quantize model](#Quantize-model)\n",
    "- [Run quantized OpenVINO model](#Run-quantized-OpenVINO-model)\n",
    "- [Interactive inference](#Interactive-inference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umcnTB7Yk_7O"
   },
   "source": [
    "## Fetch test image\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "We will use an image from a randomly sampled subset of [DocLayNet](https://github.com/DS4SD/DocLayNet) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzsHitB2yT3g"
   },
   "outputs": [],
   "source": [
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu \"openvino>=2024.1\" nncf \"transformers<=4.36.2\" \"surya-ocr==0.4.0\" torch datasets gradio Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oFUkeJKflOtF",
    "outputId": "0689aa46-3b51-4aa5-ac65-ca778c3670ae"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def fetch_image():\n",
    "    dataset = load_dataset(\"vikp/doclaynet_bench\", split=\"train\", streaming=True)\n",
    "    return next(iter(dataset))[\"image\"]\n",
    "\n",
    "\n",
    "test_image = fetch_image()\n",
    "test_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HD-qD87rScjt"
   },
   "source": [
    "## Run PyTorch inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To perform line-level text detection we will use `load_model` and `load_processor` functions from `surya` package. We will also use `batch_inference` function which performs pre and post processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdLJUj8ZS5BN"
   },
   "outputs": [],
   "source": [
    "# Predictions visualization function\n",
    "from PIL import ImageDraw\n",
    "\n",
    "\n",
    "def visualize_prediction(image, prediction):\n",
    "    image = image.copy()\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for polygon_box in prediction.bboxes:\n",
    "        draw.rectangle(polygon_box.bbox, width=1, outline=\"red\")\n",
    "\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wRbXHP5wSlIt",
    "outputId": "bd8c6997-7bcb-4c0d-c8c7-207763e55be2"
   },
   "outputs": [],
   "source": [
    "from surya.detection import batch_text_detection\n",
    "from surya.model.detection.segformer import load_model, load_processor\n",
    "\n",
    "model, processor = load_model(), load_processor()\n",
    "\n",
    "predictions = batch_text_detection([test_image], model, processor)\n",
    "\n",
    "visualize_prediction(test_image, predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WZWedJI9xEp"
   },
   "source": [
    "## Convert model to OpenVINO Intermediate Representation (IR) format\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "For best results with OpenVINO, it is recommended to convert the model to OpenVINO IR format. OpenVINO supports PyTorch via Model conversion API.\n",
    "To convert the PyTorch model to OpenVINO IR format we will use `ov.convert_model` of [model conversion API](https://docs.openvino.ai/2024/openvino-workflow/model-preparation.html). The `ov.convert_model` Python function returns an OpenVINO Model object ready to load on the device and start making predictions.\n",
    "\n",
    "`ov.convert_model` requires a sample of original model input. We will use image pre-processing from `surya` package to prepare example input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MlFO1NITS0G"
   },
   "outputs": [],
   "source": [
    "# Build example input\n",
    "from surya.input.processing import prepare_image\n",
    "import torch\n",
    "\n",
    "\n",
    "def build_example_input(image, processor):\n",
    "    input_values = prepare_image(image.convert(\"RGB\"), processor)\n",
    "\n",
    "    return {\"pixel_values\": torch.unsqueeze(input_values, 0)}\n",
    "\n",
    "\n",
    "example_input = build_example_input(test_image, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3GndhyjX-eA"
   },
   "outputs": [],
   "source": [
    "# Convert model\n",
    "import openvino as ov\n",
    "from pathlib import Path\n",
    "\n",
    "ov_model = ov.convert_model(model, example_input=example_input)\n",
    "\n",
    "FP_MODEL_PATH = Path(\"model.xml\")\n",
    "INT8_MODEL_PATH = Path(\"int8_model.xml\")\n",
    "\n",
    "ov.save_model(ov_model, FP_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run OpenVINO model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to reuse model results postprocessing implemented in `batch_inference` function. In order to do that we implement simple wrappers for OpenVINO model with interface required by `batch_inference` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_hOCui8YzZz"
   },
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "# Compile OpenVINO model for loading on device\n",
    "compiled_ov_model = core.compile_model(ov_model, device.value)\n",
    "\n",
    "\n",
    "class OVModelWrapperResult:\n",
    "    def __init__(self, logits):\n",
    "        self.logits = logits\n",
    "\n",
    "\n",
    "class OVModelWrapper:\n",
    "    dtype = torch.float32\n",
    "    device = model.device\n",
    "    config = model.config\n",
    "\n",
    "    def __init__(self, ov_model) -> None:\n",
    "        self.ov_model = ov_model\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        # run inference on preprocessed data and get image-text similarity score\n",
    "        logits = self.ov_model(kwargs)[0]\n",
    "        return OVModelWrapperResult(torch.from_numpy(logits))\n",
    "\n",
    "\n",
    "ov_model_wrapper = OVModelWrapper(compiled_ov_model)\n",
    "\n",
    "ov_predictions = batch_text_detection([test_image], ov_model_wrapper, processor)\n",
    "\n",
    "visualize_prediction(test_image, ov_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply post-training quantization using NNCF\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf/) enables post-training quantization by adding the quantization layers into the model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers. The framework is designed so that modifications to your original training code are minor. Quantization is the simplest scenario and requires a few modifications.\n",
    "\n",
    "The optimization process contains the following steps:\n",
    "\n",
    "1. Create a dataset for quantization.\n",
    "1. Run `nncf.quantize` for getting a quantized model.\n",
    "\n",
    "\n",
    "Please select below whether you would like to run quantization to improve model inference speed.\n",
    "\n",
    "> **NOTE**: Quantization is time and memory consuming operation. Running quantization code below may take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_quantize = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Quantization\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "to_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/skip_kernel_extension.py\",\n",
    ")\n",
    "open(\"skip_kernel_extension.py\", \"w\").write(r.text)\n",
    "\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free resources before quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del model\n",
    "del ov_model\n",
    "del compiled_ov_model\n",
    "del ov_model_wrapper\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "We create calibration dataset with randomly sampled set of images from [DocLayNet](https://github.com/DS4SD/DocLayNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "from surya.input.processing import split_image\n",
    "\n",
    "\n",
    "def prepare_calibration_dataset(size=1, buffer_size=1):\n",
    "\n",
    "    def collate_fn(data):\n",
    "        image = data[0][\"image\"].convert(\"RGB\")\n",
    "        image_splits, _ = split_image(image, processor)\n",
    "        image_splits = prepare_image(image_splits[0], processor)\n",
    "\n",
    "        return image_splits\n",
    "\n",
    "    dataset = load_dataset(\"vikp/doclaynet_bench\", split=\"train\", streaming=True)\n",
    "    train_dataset = dataset.shuffle(seed=42, buffer_size=buffer_size)\n",
    "    dataloader = torch.utils.data.DataLoader(train_dataset, collate_fn=collate_fn, batch_size=1)\n",
    "\n",
    "    def prepare_calibration_data(dataloader, size):\n",
    "        data = []\n",
    "        counter = 0\n",
    "        for batch in dataloader:\n",
    "            if counter == size:\n",
    "                break\n",
    "            counter += 1\n",
    "            batch = batch.to(torch.float32)\n",
    "            batch = batch.to(\"cpu\")\n",
    "            data.append({\"pixel_values\": torch.stack([batch])})\n",
    "        return data\n",
    "\n",
    "    return prepare_calibration_data(dataloader, size)\n",
    "\n",
    "\n",
    "calibration_dataset = prepare_calibration_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Create a quantized model from the `FP16` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import nncf\n",
    "\n",
    "quantized_ov_model = nncf.quantize(\n",
    "    model=core.read_model(FP_MODEL_PATH),\n",
    "    calibration_dataset=nncf.Dataset(calibration_dataset),\n",
    "    advanced_parameters=nncf.AdvancedQuantizationParameters(\n",
    "        activations_quantization_params=nncf.quantization.advanced_parameters.QuantizationParameters(per_channel=False)\n",
    "    ),\n",
    "    ignored_scope=nncf.IgnoredScope(names=[\"Multiply_28993\"]),\n",
    ")\n",
    "\n",
    "ov.save_model(quantized_ov_model, INT8_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run quantized OpenVINO model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now we ready to detect lines with `int8` OpenVINO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "# Compile OpenVINO model for loading on device\n",
    "compiled_int8_ov_model = core.compile_model(quantized_ov_model, device.value)\n",
    "\n",
    "int8_ov_model_wrapper = OVModelWrapper(compiled_int8_ov_model)\n",
    "\n",
    "int8_ov_predictions = batch_text_detection([test_image], int8_ov_model_wrapper, processor)\n",
    "\n",
    "visualize_prediction(test_image, int8_ov_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive inference\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now, it is your turn! Feel free to upload an image, using the file upload window.\n",
    "\n",
    "Below you can select which model to run: original or quantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "quantized_model_present = Path(INT8_MODEL_PATH).exists()\n",
    "\n",
    "use_quantized_model = widgets.Checkbox(\n",
    "    value=True if quantized_model_present else False,\n",
    "    description=\"Use quantized model\",\n",
    "    disabled=not quantized_model_present,\n",
    ")\n",
    "\n",
    "use_quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "compiled_model = ov.compile_model(INT8_MODEL_PATH if use_quantized_model.value else FP_MODEL_PATH, device.value)\n",
    "\n",
    "\n",
    "def predict(image):\n",
    "    predictions = batch_text_detection([image], OVModelWrapper(compiled_model), processor)\n",
    "\n",
    "    image = image.copy()\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for polygon_box in predictions[0].bboxes:\n",
    "        draw.rectangle(polygon_box.bbox, width=1, outline=\"red\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    predict,\n",
    "    gr.Image(label=\"Image\", type=\"pil\", format=\"pil\"),\n",
    "    gr.Image(label=\"Result\"),\n",
    "    examples=[test_image],\n",
    ")\n",
    "try:\n",
    "    demo.launch(debug=True, height=1000)\n",
    "except Exception:\n",
    "    demo.launch(share=True, debug=True, height=1000)\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "# demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# Read more in the docs: https://gradio.app/docs/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt.png",
   "tags": {
    "categories": [
     "AI Trends",
     "Convert",
     "Model Demos"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Text Detection"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
