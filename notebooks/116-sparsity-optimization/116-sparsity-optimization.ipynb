{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacbe6b4",
   "metadata": {
    "id": "rQc-wXjqrEuR"
   },
   "source": [
    "# Accelerate Inference of Sparse Transformer Models with OpenVINO™ and 4th Gen Intel&reg; Xeon&reg; Scalable Processors\n",
    "This tutorial demonstrates how to improve performance of sparse Transformer models with [OpenVINO](https://docs.openvino.ai/) on 4th Gen Intel® Xeon® Scalable processors. \n",
    "\n",
    "The tutorial downloads a BERT-base model which has been quantized, sparsified, and tuned for SST2 datasets using [Optimum-Intel](https://github.com/huggingface/optimum-intel). It demonstrates the inference performance advantage on 4th Gen Intel&reg; Xeon&reg; Scalable Processors by running it with [Sparse Weight Decompression](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_CPU.html#sparse-weights-decompression), a runtime option that siezes model sparsity for efficiency. The notebook consists of the following steps:\n",
    "\n",
    "- Install prerequisites\n",
    "- Download and quantize sparse BERT model from a public source using the OpenVINO integration with Hugging Face Optimum.\n",
    "- Compare sparse 8-bit vs. dense 8-bit inference performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef22e9",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc9afb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum[openvino] in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: openvino-dev in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (2022.3.0)\n",
      "Requirement already satisfied: datasets in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (2.10.1)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from optimum[openvino]) (4.26.1)\n",
      "Requirement already satisfied: coloredlogs in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from optimum[openvino]) (15.0.1)\n",
      "Requirement already satisfied: numpy in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from optimum[openvino]) (1.23.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from optimum[openvino]) (0.13.1)\n",
      "Requirement already satisfied: sympy in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from optimum[openvino]) (1.11.1)\n",
      "Requirement already satisfied: torch>=1.9 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from optimum[openvino]) (1.13.1)\n",
      "Requirement already satisfied: packaging in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from optimum[openvino]) (23.0)\n",
      "Requirement already satisfied: optimum-intel[openvino] in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from optimum[openvino]) (1.6.3)\n",
      "Requirement already satisfied: tqdm>=4.54.1 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (4.65.0)\n",
      "Requirement already satisfied: networkx<=2.8.8 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (2.8.8)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (0.7.1)\n",
      "Requirement already satisfied: openvino-telemetry>=2022.1.0 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (2022.3.0)\n",
      "Requirement already satisfied: opencv-python>=4.5 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (4.7.0.72)\n",
      "Requirement already satisfied: addict>=2.4.0 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (2.4.0)\n",
      "Requirement already satisfied: pandas~=1.3.5 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (1.3.5)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (6.0)\n",
      "Requirement already satisfied: jstyleson>=0.0.2 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (0.0.2)\n",
      "Requirement already satisfied: openvino==2022.3.0 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (2022.3.0)\n",
      "Requirement already satisfied: pillow>=8.1.2 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (9.4.0)\n",
      "Requirement already satisfied: texttable>=1.6.3 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (1.6.7)\n",
      "Requirement already satisfied: scipy>=1.8 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (1.10.1)\n",
      "Requirement already satisfied: requests>=2.25.1 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from openvino-dev) (2.28.2)\n",
      "Requirement already satisfied: aiohttp in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: xxhash in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (4.5.0)\n",
      "Requirement already satisfied: filelock in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (3.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from pandas~=1.3.5->openvino-dev) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from pandas~=1.3.5->openvino-dev) (2022.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from requests>=2.25.1->openvino-dev) (2022.12.7)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from torch>=1.9->optimum[openvino]) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from torch>=1.9->optimum[openvino]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from torch>=1.9->optimum[openvino]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from torch>=1.9->optimum[openvino]) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->optimum[openvino]) (65.6.3)\n",
      "Requirement already satisfied: wheel in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->optimum[openvino]) (0.38.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[openvino]) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[openvino]) (0.13.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[openvino]) (0.1.97)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[openvino]) (3.20.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from coloredlogs->optimum[openvino]) (10.0)\n",
      "Requirement already satisfied: onnxruntime in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from optimum-intel[openvino]->optimum[openvino]) (1.14.1)\n",
      "Requirement already satisfied: onnx in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from optimum-intel[openvino]->optimum[openvino]) (1.13.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from sympy->optimum[openvino]) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas~=1.3.5->openvino-dev) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers in /root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages (from onnxruntime->optimum-intel[openvino]->optimum[openvino]) (23.3.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install optimum[openvino] openvino-dev datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b41e6-132b-40da-b3b9-91bacba29e31",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771388d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/vchua/miniconda3/envs/ov-nb/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from optimum.intel.openvino import OVModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603a481",
   "metadata": {},
   "source": [
    "### Download a quantized sparse the model using Hugging Face Optimum API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a1020-0c12-4f3d-ae9f-9464d2e18b10",
   "metadata": {},
   "source": [
    "The first step is to download a quantized sparse transformers which has been translated to OpenVINO IR. Then, it will be put through a classification, for simple validation of a working downloaded model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b897c926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|████████████████████████████████████████████████████████| 840/840 [00:00<00:00, 315kB/s]\n",
      "Downloading (…)n/openvino_model.xml: 100%|███████████████████████████████████████████████████| 1.35M/1.35M [00:00<00:00, 3.39MB/s]\n",
      "Downloading openvino_model.bin: 100%|██████████████████████████████████████████████████████████| 182M/182M [00:03<00:00, 50.1MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████████████████| 571/571 [00:00<00:00, 150kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|█████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 1.20MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|█████████████████████████████████████████████████████| 711k/711k [00:00<00:00, 2.18MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 137kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative', 'score': 0.9981877207756042}]\n"
     ]
    }
   ],
   "source": [
    "# The following model has been quantized, sparsified using Optimum-Intel 1.7 which is enabled by OpenVINO and NNCF\n",
    "# for reproducibility, refer https://huggingface.co/OpenVINO/bert-base-uncased-sst2-int8-unstructured80\n",
    "model_id = \"OpenVINO/bert-base-uncased-sst2-int8-unstructured80\"\n",
    "\n",
    "# The following two steps will set up the model and download them to HF Cache folder\n",
    "ov_model = OVModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Let's take the model for a spin!\n",
    "sentiment_classifier = pipeline(\"text-classification\", model=ov_model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"He's a dreadful magician.\"\n",
    "outputs = sentiment_classifier(text)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d97eb-e91e-48bb-bba9-30598e49bb4f",
   "metadata": {},
   "source": [
    "For benchmarking, we will use OpenVINO's benchmark application and let's put the IRs into a single folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2caade15-cb72-4d49-8400-21ce56b9c220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_80pc_sparse_quantized_ir/openvino_model.bin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a folder\n",
    "quantized_sparse_dir = Path(\"bert_80pc_sparse_quantized_ir\")\n",
    "quantized_sparse_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# following return path to specified filename in cache folder and download if not found\n",
    "ov_ir_xml_path = hf_hub_download(repo_id=model_id, filename=\"openvino_model.xml\")\n",
    "ov_ir_bin_path = hf_hub_download(repo_id=model_id, filename=\"openvino_model.bin\")\n",
    "\n",
    "# copy IRs to our folder\n",
    "shutil.copy(ov_ir_xml_path, quantized_sparse_dir)\n",
    "shutil.copy(ov_ir_bin_path, quantized_sparse_dir)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6830eb7",
   "metadata": {},
   "source": [
    "## Benchmark quantized dense inference performance\n",
    "Benchmark dense inference performance using parallel execution on four CPU cores to simulate a small instance in the cloud infrastructure. Sequense length is set to 16 which is common for multiple use cases, e.g. conversational AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa895f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dump benchmarking config for dense inference\n",
    "with open(f\"{quantized_sparse_dir}/perf_config.json\", \"w\") as outfile:\n",
    "    outfile.write(\n",
    "        \"\"\"\n",
    "        {\n",
    "            \"CPU\": {\"NUM_STREAMS\": 4, \"INFERENCE_NUM_THREADS\": 4}\n",
    "        }\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f6c7526",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 130.04 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : i64 / [...] / [?,?]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: logits) : f32 / [...] / [?,2]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input_ids': [1,16], 'attention_mask': [1,16], 'token_type_ids': [1,16]\n",
      "[ INFO ] Reshape model took 26.49 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [1,16]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : i64 / [...] / [1,16]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [1,16]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: logits) : f32 / [...] / [1,2]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 1276.96 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4\n",
      "[ INFO ]   NUM_STREAMS: 4\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 4\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'bfloat16'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input_ids'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'attention_mask'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'token_type_ids'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input_ids' with random values \n",
      "[ INFO ] Fill input 'attention_mask' with random values \n",
      "[ INFO ] Fill input 'token_type_ids' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 9.46 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            34096 iterations\n",
      "[ INFO ] Duration:         60012.62 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        5.93 ms\n",
      "[ INFO ]    Average:       7.03 ms\n",
      "[ INFO ]    Min:           4.85 ms\n",
      "[ INFO ]    Max:           32.04 ms\n",
      "[ INFO ] Throughput:   568.15 FPS\n"
     ]
    }
   ],
   "source": [
    "!(benchmark_app \\\n",
    "  -m bert_80pc_sparse_quantized_ir/openvino_model.xml \\\n",
    "  -shape \"input_ids[1,16],attention_mask[1,16],token_type_ids[1,16]\" \\\n",
    "  -load_config bert_80pc_sparse_quantized_ir/perf_config.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9151b11",
   "metadata": {},
   "source": [
    "## Benchmark quantized sparse inference performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c305661a-27a5-45aa-b3df-777862584adf",
   "metadata": {},
   "source": [
    "To enable sparse weight decompression feature, users can add it to runtime config like below. `CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE` honors value between 0.5 and 1.0, it is a layer-level sparsity threshold for which a layer will be enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad77ae5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dump benchmarking config for dense inference\n",
    "# \"CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE\" controls minimum sparsity rate for weights to consider \n",
    "# for sparse optimization at the runtime.\n",
    "with open(f\"{quantized_sparse_dir}/perf_config_sparse.json\", \"w\") as outfile:\n",
    "    outfile.write(\n",
    "        \"\"\"\n",
    "        {\n",
    "            \"CPU\": {\"NUM_STREAMS\": 4, \"INFERENCE_NUM_THREADS\": 4, \"CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE\": 0.75}\n",
    "        }\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ddd8b10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 142.44 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : i64 / [...] / [?,?]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: logits) : f32 / [...] / [?,2]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input_ids': [1,16], 'attention_mask': [1,16], 'token_type_ids': [1,16]\n",
      "[ INFO ] Reshape model took 26.55 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [1,16]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : i64 / [...] / [1,16]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [1,16]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: logits) : f32 / [...] / [1,2]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 1376.07 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4\n",
      "[ INFO ]   NUM_STREAMS: 4\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 4\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'bfloat16'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input_ids'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'attention_mask'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'token_type_ids'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input_ids' with random values \n",
      "[ INFO ] Fill input 'attention_mask' with random values \n",
      "[ INFO ] Fill input 'token_type_ids' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 7.66 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            50084 iterations\n",
      "[ INFO ] Duration:         60006.95 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        4.72 ms\n",
      "[ INFO ]    Average:       4.78 ms\n",
      "[ INFO ]    Min:           4.23 ms\n",
      "[ INFO ]    Max:           15.20 ms\n",
      "[ INFO ] Throughput:   834.64 FPS\n"
     ]
    }
   ],
   "source": [
    "!(benchmark_app \\\n",
    "  -m bert_80pc_sparse_quantized_ir/openvino_model.xml \\\n",
    "  -shape \"input_ids[1,16],attention_mask[1,16],token_type_ids[1,16]\" \\\n",
    "  -load_config bert_80pc_sparse_quantized_ir/perf_config_sparse.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d4d61",
   "metadata": {},
   "source": [
    "## When this might be helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c8526",
   "metadata": {},
   "source": [
    "This feauture can improve inference performance for models with sparse weights in the scenarios when the model is deployed to handle multiple requests in parallel asyncronously. It is especially helpful in the case of small sequence length, e.g. 32 and lower.\n",
    "\n",
    "For more details about asynchronous inference with OpenVINO please refer to the following documentation:\n",
    "- [Deployment Optimization Guide](https://docs.openvino.ai/latest/openvino_docs_deployment_optimization_guide_common.html#doxid-openvino-docs-deployment-optimization-guide-common-1async-api)\n",
    "- [Inference Request API](https://docs.openvino.ai/latest/openvino_docs_OV_UG_Infer_request.html#doxid-openvino-docs-o-v-u-g-infer-request-1in-out-tensors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe409241748dff4afe127d33bbdaaa11b54ce82261ed669fd8a0538ea98c62f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
